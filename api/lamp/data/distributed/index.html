<!DOCTYPE html ><html><head><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport"/><title></title><meta content="" name="description"/><meta content="" name="keywords"/><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><link href="../../../lib/index.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../lib/template.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../lib/print.css" media="print" type="text/css" rel="stylesheet"/><link href="../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css"/><script type="text/javascript" src="../../../lib/jquery.min.js"></script><script type="text/javascript" src="../../../lib/index.js"></script><script type="text/javascript" src="../../../index.js"></script><script type="text/javascript" src="../../../lib/scheduler.js"></script><script type="text/javascript" src="../../../lib/template.js"></script><script type="text/javascript">/* this variable can be used by the JS to determine the path to the root document */
var toRoot = '../../../';</script></head><body><div id="search"><span id="doc-title"><span id="doc-version"></span></span> <span class="close-results"><span class="left">&lt;</span> Back</span><div id="textfilter"><span class="input"><input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/"/><i class="clear material-icons"></i><i id="search-icon" class="material-icons"></i></span></div></div><div id="search-results"><div id="search-progress"><div id="progress-fill"></div></div><div id="results-content"><div id="entity-results"></div><div id="member-results"></div></div></div><div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;"><div id="content-container" style="-webkit-overflow-scrolling: touch;"><div id="subpackage-spacer"><div id="packages"><h1>Packages</h1><ul><li class="indented0 " name="_root_.root" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="_root_" class="anchorToMember"></a><a id="root:_root_" class="anchorToMember"></a> <span class="permalink"><a href="../../../index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../index.html" title=""><span class="name">root</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented1 " name="_root_.lamp" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="lamp" class="anchorToMember"></a><a id="lamp:lamp" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../index.html" title="Lamp provides utilities to build state of the art machine learning applications"><span class="name">lamp</span></a></span><p class="shortcomment cmt">Lamp provides utilities to build state of the art machine learning
applications</p><div class="fullcomment"><div class="comment cmt"><p>Lamp provides utilities to build state of the art machine learning
applications</p><h3>Overview</h3><p>Notable types and packages:</p><ul><li><a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">lamp.STen</a> is a memory managed wrapper around aten.ATen, an off the
    heap, native n-dimensionl array backed by libtorch.</li><li><a href="../../autograd/index.html" name="lamp.autograd" id="lamp.autograd" class="extype">lamp.autograd</a> implements reverse mode automatic differentiation.</li><li><a href="../../nn/index.html" name="lamp.nn" id="lamp.nn" class="extype">lamp.nn</a> contains neural network building blocks, see e.g.
    <a href="../../nn/Linear.html" name="lamp.nn.Linear" id="lamp.nn.Linear" class="extype">lamp.nn.Linear</a>.</li><li><a href="../IOLoops$.html" name="lamp.data.IOLoops" id="lamp.data.IOLoops" class="extype">lamp.data.IOLoops</a> implements a training loop and other data related
    abstractions.</li><li><a href="../../knn/index.html" name="lamp.knn" id="lamp.knn" class="extype">lamp.knn</a> implements k-nearest neighbor search on the CPU and GPU</li><li><a href="../../umap/Umap$.html" name="lamp.umap.Umap" id="lamp.umap.Umap" class="extype">lamp.umap.Umap</a> implements the UMAP dimension reduction algorithm</li><li><a href="../../onnx/index.html" name="lamp.onnx" id="lamp.onnx" class="extype">lamp.onnx</a> implements serialization of computation graphs into ONNX
    format</li><li>lamp.io contains CSV and NPY readers</li></ul><h5>How to get data into lamp</h5><p>Use one of the file readers in lamp.io or one of the factories in
<a href="../../STen$.html" name="lamp.STen" id="lamp.STen" class="extype">lamp.STen$</a>.</p><h5>How to define a custom neural network layer</h5><p>See the documentation on <a href="../../nn/GenericModule.html" name="lamp.nn.GenericModule" id="lamp.nn.GenericModule" class="extype">lamp.nn.GenericModule</a></p><h5>How to compose neural network layers</h5><p>See the documentation on <a href="../../nn/index.html" name="lamp.nn" id="lamp.nn" class="extype">lamp.nn</a></p><h5>How to train models</h5><p>See the training loops in <a href="../IOLoops$.html" name="lamp.data.IOLoops" id="lamp.data.IOLoops" class="extype">lamp.data.IOLoops</a>
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented2 " name="lamp.data" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="data" class="anchorToMember"></a><a id="data:data" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../index.html" title=""><span class="name">data</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../index.html" name="lamp" id="lamp" class="extype">lamp</a></dd></dl></div></li><li class="indented3 current" name="lamp.data.distributed" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="distributed" class="anchorToMember"></a><a id="distributed:distributed" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">distributed</span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="lamp.data" id="lamp.data" class="extype">data</a></dd></dl></div></li><li class="current-entities indented3"><span class="separator"></span> <a href="DistributedCommunication$.html" title="" class="object"></a><a href="DistributedCommunication$.html" title="">DistributedCommunication</a></li><li class="current-entities indented3"><span class="separator"></span> <a href="DistributedCommunicationNonRoot.html" title="" class="trait"></a><a href="DistributedCommunicationNonRoot.html" title="">DistributedCommunicationNonRoot</a></li><li class="current-entities indented3"><span class="separator"></span> <a href="DistributedCommunicationRoot.html" title="" class="trait"></a><a href="DistributedCommunicationRoot.html" title="">DistributedCommunicationRoot</a></li><li class="current-entities indented3"><span class="separator"></span> <a href="LocalCommunication$.html" title="" class="object"></a><a href="LocalCommunication$.html" title="">LocalCommunication</a></li><li class="current-entities indented3"><a href="LoopState$.html" title="" class="object"></a> <a href="LoopState.html" title="" class="class"></a><a href="LoopState.html" title="">LoopState</a></li><li class="indented3 " name="lamp.data.schemas" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="schemas" class="anchorToMember"></a><a id="schemas:schemas" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/schemas/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../schemas/index.html" title=""><span class="name">schemas</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="lamp.data" id="lamp.data" class="extype">data</a></dd></dl></div></li></ul></div></div><div id="content"><body class="package value"><div id="definition"><div class="big-circle package">p</div><p id="owner"><a href="../../index.html" name="lamp" id="lamp" class="extype">lamp</a>.<a href="../index.html" name="lamp.data" id="lamp.data" class="extype">data</a></p><h1>distributed<span class="permalink"><a href="../../../lamp/data/distributed/index.html" title="Permalink"><i class="material-icons"></i></a></span></h1></div><h4 id="signature" class="signature"><span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">distributed</span></span></h4><div id="comment" class="fullcommenttop"><div class="toggleContainer"><div class="toggle block"><span>Linear Supertypes</span><div class="superTypes hiddenContent"><span name="scala.AnyRef" class="extype">AnyRef</span>, <span name="scala.Any" class="extype">Any</span></div></div></div></div><div id="mbrsel"><div class="toggle"></div><div id="memberfilter"><i class="material-icons arrow"></i><span class="input"><input placeholder="Filter all members" id="mbrsel-input" type="text" accesskey="/"/></span><i class="clear material-icons"></i></div><div id="filterby"><div id="order"><span class="filtertype">Ordering</span><ol><li class="alpha in"><span>Alphabetic</span></li><li class="inherit out"><span>By Inheritance</span></li></ol></div><div class="ancestors"><span class="filtertype">Inherited<br/></span><ol id="linearization"><li class="in" name="lamp.data.distributed"><span>distributed</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li></ol></div><div class="ancestors"><span class="filtertype"></span><ol><li class="hideall out"><span>Hide All</span></li><li class="showall in"><span>Show All</span></li></ol></div><div id="visbl"><span class="filtertype">Visibility</span><ol><li class="public in"><span>Public</span></li><li class="protected out"><span>Protected</span></li></ol></div></div></div><div id="template"><div id="allMembers"><div id="types" class="types members"><h3>Type Members</h3><ol><li class="indented0 " name="lamp.data.distributed.DistributedCommunicationNonRoot" group="Ungrouped" fullComment="no" data-isabs="true" visbl="pub"><a id="DistributedCommunicationNonRootextendsAnyRef" class="anchorToMember"></a><a id="DistributedCommunicationNonRoot:DistributedCommunicationNonRoot" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/DistributedCommunicationNonRoot.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="DistributedCommunicationNonRoot.html" title=""><span class="name">DistributedCommunicationNonRoot</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span></li><li class="indented0 " name="lamp.data.distributed.DistributedCommunicationRoot" group="Ungrouped" fullComment="no" data-isabs="true" visbl="pub"><a id="DistributedCommunicationRootextendsAnyRef" class="anchorToMember"></a><a id="DistributedCommunicationRoot:DistributedCommunicationRoot" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/DistributedCommunicationRoot.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="DistributedCommunicationRoot.html" title=""><span class="name">DistributedCommunicationRoot</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span></li><li class="indented0 " name="lamp.data.distributed.LoopState" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="LoopStateextendsProductwithSerializable" class="anchorToMember"></a><a id="LoopState:LoopState" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/LoopState.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">case class</span></span> <span class="symbol"><a href="LoopState.html" title=""><span class="name">LoopState</span></a><span class="params">(<span name="epoch">epoch: <span name="scala.Int" class="extype">Int</span></span>, <span name="lastValidationLoss">lastValidationLoss: <span name="scala.Option" class="extype">Option</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="minValidationLoss">minValidationLoss: <span name="scala.Option" class="extype">Option</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="minValidationEpoch">minValidationEpoch: <span name="scala.Option" class="extype">Option</span>[<span name="scala.Int" class="extype">Int</span>]</span>, <span name="learningCurve">learningCurve: <span name="scala.List" class="extype">List</span>[(<span name="scala.Int" class="extype">Int</span>, <span name="scala.Double" class="extype">Double</span>, <span name="scala.Option" class="extype">Option</span>[(<span name="scala.Double" class="extype">Double</span>, <span name="scala.Double" class="extype">Double</span>)])]</span>)</span><span class="result"> extends <span name="scala.Product" class="extype">Product</span> with <span name="scala.Serializable" class="extype">Serializable</span></span></span></li></ol></div><div class="values members"><h3>Value Members</h3><ol><li class="indented0 " name="lamp.data.distributed#driveDistributedTraining" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="driveDistributedTraining[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](nranks:Int,gpu:Int,controlCommunication:lamp.data.distributed.DistributedCommunicationRoot,model:lamp.nn.SupervisedModel[I,M],optimizerFactory:Seq[(lamp.STen,lamp.nn.PTag)]=&gt;lamp.nn.Optimizer,trainBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],validationBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],accumulateGradientOverNBatches:Int,learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double):cats.effect.IO[lamp.data.distributed.LoopState]" class="anchorToMember"></a><a id="driveDistributedTraining[I,M&lt;:GenericModule[I,Variable],LRState,BatchStreamState,BatchStreamBuffers](Int,Int,DistributedCommunicationRoot,SupervisedModel[I,M],(Seq[(STen,PTag)])=&gt;Optimizer,()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers],()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers],Int,Option[(LoopState,LRState)=&gt;IO[Unit]],Int,Seq[Int],LearningRateSchedule[LRState],Option[LoopState],Int,Option[LRState],Double):IO[LoopState]" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html#driveDistributedTraining[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](nranks:Int,gpu:Int,controlCommunication:lamp.data.distributed.DistributedCommunicationRoot,model:lamp.nn.SupervisedModel[I,M],optimizerFactory:Seq[(lamp.STen,lamp.nn.PTag)]=&gt;lamp.nn.Optimizer,trainBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],validationBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],accumulateGradientOverNBatches:Int,learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double):cats.effect.IO[lamp.data.distributed.LoopState]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">driveDistributedTraining</span><span class="tparams">[<span name="I">I</span>, <span name="M">M &lt;: <a href="../../nn/GenericModule.html" name="lamp.nn.GenericModule" id="lamp.nn.GenericModule" class="extype">GenericModule</a>[<span name="lamp.data.distributed.driveDistributedTraining.I" class="extype">I</span>, <a href="../../autograd/Variable.html" name="lamp.autograd.Variable" id="lamp.autograd.Variable" class="extype">Variable</a>]</span>, <span name="LRState">LRState</span>, <span name="BatchStreamState">BatchStreamState</span>, <span name="BatchStreamBuffers">BatchStreamBuffers</span>]</span><span class="params">(<span name="nranks">nranks: <span name="scala.Int" class="extype">Int</span></span>, <span name="gpu">gpu: <span name="scala.Int" class="extype">Int</span></span>, <span name="controlCommunication">controlCommunication: <a href="DistributedCommunicationRoot.html" name="lamp.data.distributed.DistributedCommunicationRoot" id="lamp.data.distributed.DistributedCommunicationRoot" class="extype">DistributedCommunicationRoot</a></span>, <span name="model">model: <a href="../../nn/SupervisedModel.html" name="lamp.nn.SupervisedModel" id="lamp.nn.SupervisedModel" class="extype">SupervisedModel</a>[<span name="lamp.data.distributed.driveDistributedTraining.I" class="extype">I</span>, <span name="lamp.data.distributed.driveDistributedTraining.M" class="extype">M</span>]</span>, <span name="optimizerFactory">optimizerFactory: (<span name="scala.Seq" class="extype">Seq</span>[(<a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>, <a href="../../nn/PTag.html" name="lamp.nn.PTag" id="lamp.nn.PTag" class="extype">PTag</a>)]) =&gt; <a href="../../nn/Optimizer.html" name="lamp.nn.Optimizer" id="lamp.nn.Optimizer" class="extype">Optimizer</a></span>, <span name="trainBatches">trainBatches: () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.driveDistributedTraining.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.driveDistributedTraining.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.driveDistributedTraining.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>]</span>, <span name="validationBatches">validationBatches: () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.driveDistributedTraining.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.driveDistributedTraining.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.driveDistributedTraining.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>]</span>, <span name="maxEpochs">maxEpochs: <span name="scala.Int" class="extype">Int</span></span>, <span name="checkpointState">checkpointState: <span name="scala.Option" class="extype">Option</span>[(<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>, <span name="lamp.data.distributed.driveDistributedTraining.LRState" class="extype">LRState</span>) =&gt; <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Unit" class="extype">Unit</span>]] = <span class="symbol">None</span></span>, <span name="validationFrequency">validationFrequency: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>, <span name="returnMinValidationLossModel">returnMinValidationLossModel: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Int" class="extype">Int</span>] = <span class="symbol">Nil</span></span>, <span name="learningRateSchedule">learningRateSchedule: <a href="../../nn/LearningRateSchedule.html" name="lamp.nn.LearningRateSchedule" id="lamp.nn.LearningRateSchedule" class="extype">LearningRateSchedule</a>[<span name="lamp.data.distributed.driveDistributedTraining.LRState" class="extype">LRState</span>] = <span class="symbol"><span class="name"><a href="../../index.html">LearningRateSchedule.noop</a></span></span></span>, <span name="initState">initState: <span name="scala.Option" class="extype">Option</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>] = <span class="symbol">None</span></span>, <span name="accumulateGradientOverNBatches">accumulateGradientOverNBatches: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>, <span name="learningRateScheduleInitState">learningRateScheduleInitState: <span name="scala.Option" class="extype">Option</span>[<span name="lamp.data.distributed.driveDistributedTraining.LRState" class="extype">LRState</span>] = <span class="symbol">None</span></span>, <span name="validationLossExponentialSmoothingFactor">validationLossExponentialSmoothingFactor: <span name="scala.Double" class="extype">Double</span> = <span class="symbol">1.0</span></span>)</span><span class="result">: <span name="cats.effect.IO" class="extype">IO</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>]</span></span><p class="shortcomment cmt">Drives the distributed training loop.</p><div class="fullcomment"><div class="comment cmt"><p>Drives the distributed training loop.</p><p>Must be called on the root rank. If nranks is &gt; 1 then
followDistributedTraining must be called on the rest of the ranks.</p><p>The batch streams across all ranks must:</p><ul><li>not contain empty batches</li><li>have the same number of batches.</li></ul><p>Models across all ranks must have the same shape.</p><p>Communication is done by two independent communication channels:</p><ul><li>tensor data is sent via NCCL, thus NCCL's requirement for network
    setup applies (i.e. single private network if distributed) This method
    will set up and tear down the NCCL communication clique.</li><li>control messages and initial rendez-vous are using an implementation
    of DistributedCommunicationRoot and DistributedCommunicationNonRoot.
    This is a very low traffic channel, 1 message before each epoch. An
    Akka implementation is provided which is suitable for distributed and
    single-process multi-gpu settings. A within process cats effect
    implementation is also provided for single-process multi-gpu settings.</li></ul><p>For single process multi gpu settings lamp provides two mutually exclusive
alternatives:</p><ul><li>The training loop in IOLoops (see the <code>dataParallelModels</code> argument of
    IOLoops.epochs).</li><li>???
</li></ul></div></div></li><li class="indented0 " name="lamp.data.distributed#epochs" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="epochs[LRState](maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double,trainEpoch:Double=&gt;cats.effect.IO[Double],validationEpoch:Option[cats.effect.IO[Double]],saveModel:cats.effect.IO[Unit]):cats.effect.IO[lamp.data.distributed.LoopState]" class="anchorToMember"></a><a id="epochs[LRState](Int,Option[(LoopState,LRState)=&gt;IO[Unit]],Int,Seq[Int],LearningRateSchedule[LRState],Option[LoopState],Option[LRState],Double,(Double)=&gt;IO[Double],Option[IO[Double]],IO[Unit]):IO[LoopState]" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html#epochs[LRState](maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double,trainEpoch:Double=&gt;cats.effect.IO[Double],validationEpoch:Option[cats.effect.IO[Double]],saveModel:cats.effect.IO[Unit]):cats.effect.IO[lamp.data.distributed.LoopState]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">epochs</span><span class="tparams">[<span name="LRState">LRState</span>]</span><span class="params">(<span name="maxEpochs">maxEpochs: <span name="scala.Int" class="extype">Int</span></span>, <span name="checkpointState">checkpointState: <span name="scala.Option" class="extype">Option</span>[(<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>, <span name="lamp.data.distributed.epochs.LRState" class="extype">LRState</span>) =&gt; <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Unit" class="extype">Unit</span>]] = <span class="symbol">None</span></span>, <span name="validationFrequency">validationFrequency: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>, <span name="returnMinValidationLossModel">returnMinValidationLossModel: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Int" class="extype">Int</span>] = <span class="symbol">Nil</span></span>, <span name="learningRateSchedule">learningRateSchedule: <a href="../../nn/LearningRateSchedule.html" name="lamp.nn.LearningRateSchedule" id="lamp.nn.LearningRateSchedule" class="extype">LearningRateSchedule</a>[<span name="lamp.data.distributed.epochs.LRState" class="extype">LRState</span>] = <span class="symbol"><span class="name"><a href="../../index.html">LearningRateSchedule.noop</a></span></span></span>, <span name="initState">initState: <span name="scala.Option" class="extype">Option</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>] = <span class="symbol">None</span></span>, <span name="learningRateScheduleInitState">learningRateScheduleInitState: <span name="scala.Option" class="extype">Option</span>[<span name="lamp.data.distributed.epochs.LRState" class="extype">LRState</span>] = <span class="symbol">None</span></span>, <span name="validationLossExponentialSmoothingFactor">validationLossExponentialSmoothingFactor: <span name="scala.Double" class="extype">Double</span> = <span class="symbol">1.0</span></span>, <span name="trainEpoch">trainEpoch: (<span name="scala.Double" class="extype">Double</span>) =&gt; <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="validationEpoch">validationEpoch: <span name="scala.Option" class="extype">Option</span>[<span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Double" class="extype">Double</span>]]</span>, <span name="saveModel">saveModel: <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Unit" class="extype">Unit</span>]</span>)</span><span class="result">: <span name="cats.effect.IO" class="extype">IO</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>]</span></span><p class="shortcomment cmt">Drives multiple epochs to find the minimum of smoothed validation loss</p><div class="fullcomment"><div class="comment cmt"><p>Drives multiple epochs to find the minimum of smoothed validation loss</p><p>This method does not explicitly trains a model but assumes there is a side
effecting effectful function which steps through an optimizer through a
whole epoch worth of batches.
</p></div><dl class="paramcmts block"><dt class="param">checkpointState</dt><dd class="cmt"><p>
  Function to checkpoint the state managed in this loop.</p></dd><dt class="param">validationFrequency</dt><dd class="cmt"><p>
  How often (by epoch count) to calculate the validation loss</p></dd><dt class="param">returnMinValidationLossModel</dt><dd class="cmt"><p>
  In which epocchs to calculat validation loss</p></dd><dt class="param">initState</dt><dd class="cmt"><p>
  Initial state of the validation loss management state</p></dd><dt class="param">learningRateScheduleInitState</dt><dd class="cmt"><p>
  Initial state of the learning rate state</p></dd><dt class="param">validationLossExponentialSmoothingFactor</dt><dd class="cmt"><p>
  Smoothing factor in exponential smoothing of validation loss &lt;= 1.0</p></dd><dt class="param">trainEpoch</dt><dd class="cmt"><p>
  An effectful function which steps the optimizer over a complete epoch
  and returns the training loss</p></dd><dt class="param">validationEpoch</dt><dd class="cmt"><p>
  An effectful function which steps through in forward mode a complete
  epoch and returns the validation loss</p></dd><dt class="param">saveModel</dt><dd class="cmt"><p>
  A side effect to save the current optimizer and model states</p></dd><dt>returns</dt><dd class="cmt"><p>
  The final loop state</p></dd></dl></div></li><li class="indented0 " name="lamp.data.distributed#followDistributedTraining" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="followDistributedTraining[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](rank:Int,nranks:Int,gpu:Int,controlCommunication:lamp.data.distributed.DistributedCommunicationNonRoot,model:lamp.nn.SupervisedModel[I,M],trainBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],validationBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],accumulateGradientOverNBatches:Int):cats.effect.IO[Unit]" class="anchorToMember"></a><a id="followDistributedTraining[I,M&lt;:GenericModule[I,Variable],LRState,BatchStreamState,BatchStreamBuffers](Int,Int,Int,DistributedCommunicationNonRoot,SupervisedModel[I,M],()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers],()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers],Int):IO[Unit]" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html#followDistributedTraining[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](rank:Int,nranks:Int,gpu:Int,controlCommunication:lamp.data.distributed.DistributedCommunicationNonRoot,model:lamp.nn.SupervisedModel[I,M],trainBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],validationBatches:()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],accumulateGradientOverNBatches:Int):cats.effect.IO[Unit]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">followDistributedTraining</span><span class="tparams">[<span name="I">I</span>, <span name="M">M &lt;: <a href="../../nn/GenericModule.html" name="lamp.nn.GenericModule" id="lamp.nn.GenericModule" class="extype">GenericModule</a>[<span name="lamp.data.distributed.followDistributedTraining.I" class="extype">I</span>, <a href="../../autograd/Variable.html" name="lamp.autograd.Variable" id="lamp.autograd.Variable" class="extype">Variable</a>]</span>, <span name="LRState">LRState</span>, <span name="BatchStreamState">BatchStreamState</span>, <span name="BatchStreamBuffers">BatchStreamBuffers</span>]</span><span class="params">(<span name="rank">rank: <span name="scala.Int" class="extype">Int</span></span>, <span name="nranks">nranks: <span name="scala.Int" class="extype">Int</span></span>, <span name="gpu">gpu: <span name="scala.Int" class="extype">Int</span></span>, <span name="controlCommunication">controlCommunication: <a href="DistributedCommunicationNonRoot.html" name="lamp.data.distributed.DistributedCommunicationNonRoot" id="lamp.data.distributed.DistributedCommunicationNonRoot" class="extype">DistributedCommunicationNonRoot</a></span>, <span name="model">model: <a href="../../nn/SupervisedModel.html" name="lamp.nn.SupervisedModel" id="lamp.nn.SupervisedModel" class="extype">SupervisedModel</a>[<span name="lamp.data.distributed.followDistributedTraining.I" class="extype">I</span>, <span name="lamp.data.distributed.followDistributedTraining.M" class="extype">M</span>]</span>, <span name="trainBatches">trainBatches: () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.followDistributedTraining.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.followDistributedTraining.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.followDistributedTraining.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>]</span>, <span name="validationBatches">validationBatches: () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.followDistributedTraining.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.followDistributedTraining.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.followDistributedTraining.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>]</span>, <span name="accumulateGradientOverNBatches">accumulateGradientOverNBatches: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>)</span><span class="result">: <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Unit" class="extype">Unit</span>]</span></span><p class="shortcomment cmt">Follows a distributed training loop.</p><div class="fullcomment"><div class="comment cmt"><p>Follows a distributed training loop. See the documentation of
driveDistributedTraining.
</p></div></div></li><li class="indented0 " name="lamp.data.distributed#localDataParallelTrainingLoop" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="localDataParallelTrainingLoop[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](modelsWithDataStreams:Seq[(lamp.nn.SupervisedModel[I,M],()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers])],optimizerFactory:Seq[(lamp.STen,lamp.nn.PTag)]=&gt;lamp.nn.Optimizer,maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],accumulateGradientOverNBatches:Int,learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double):cats.effect.IO[lamp.data.distributed.LoopState]" class="anchorToMember"></a><a id="localDataParallelTrainingLoop[I,M&lt;:GenericModule[I,Variable],LRState,BatchStreamState,BatchStreamBuffers](Seq[(SupervisedModel[I,M],()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers],()=&gt;BatchStream[(I,STen),BatchStreamState,BatchStreamBuffers])],(Seq[(STen,PTag)])=&gt;Optimizer,Int,Option[(LoopState,LRState)=&gt;IO[Unit]],Int,Seq[Int],LearningRateSchedule[LRState],Option[LoopState],Int,Option[LRState],Double):IO[LoopState]" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html#localDataParallelTrainingLoop[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],LRState,BatchStreamState,BatchStreamBuffers](modelsWithDataStreams:Seq[(lamp.nn.SupervisedModel[I,M],()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers],()=&gt;lamp.data.BatchStream[(I,lamp.STen),BatchStreamState,BatchStreamBuffers])],optimizerFactory:Seq[(lamp.STen,lamp.nn.PTag)]=&gt;lamp.nn.Optimizer,maxEpochs:Int,checkpointState:Option[(lamp.data.distributed.LoopState,LRState)=&gt;cats.effect.IO[Unit]],validationFrequency:Int,returnMinValidationLossModel:Seq[Int],learningRateSchedule:lamp.nn.LearningRateSchedule[LRState],initState:Option[lamp.data.distributed.LoopState],accumulateGradientOverNBatches:Int,learningRateScheduleInitState:Option[LRState],validationLossExponentialSmoothingFactor:Double):cats.effect.IO[lamp.data.distributed.LoopState]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">localDataParallelTrainingLoop</span><span class="tparams">[<span name="I">I</span>, <span name="M">M &lt;: <a href="../../nn/GenericModule.html" name="lamp.nn.GenericModule" id="lamp.nn.GenericModule" class="extype">GenericModule</a>[<span name="lamp.data.distributed.localDataParallelTrainingLoop.I" class="extype">I</span>, <a href="../../autograd/Variable.html" name="lamp.autograd.Variable" id="lamp.autograd.Variable" class="extype">Variable</a>]</span>, <span name="LRState">LRState</span>, <span name="BatchStreamState">BatchStreamState</span>, <span name="BatchStreamBuffers">BatchStreamBuffers</span>]</span><span class="params">(<span name="modelsWithDataStreams">modelsWithDataStreams: <span name="scala.Seq" class="extype">Seq</span>[(<a href="../../nn/SupervisedModel.html" name="lamp.nn.SupervisedModel" id="lamp.nn.SupervisedModel" class="extype">SupervisedModel</a>[<span name="lamp.data.distributed.localDataParallelTrainingLoop.I" class="extype">I</span>, <span name="lamp.data.distributed.localDataParallelTrainingLoop.M" class="extype">M</span>], () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.localDataParallelTrainingLoop.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.localDataParallelTrainingLoop.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.localDataParallelTrainingLoop.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>], () =&gt; <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.localDataParallelTrainingLoop.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.localDataParallelTrainingLoop.BatchStreamState" class="extype">BatchStreamState</span>, <span name="lamp.data.distributed.localDataParallelTrainingLoop.BatchStreamBuffers" class="extype">BatchStreamBuffers</span>])]</span>, <span name="optimizerFactory">optimizerFactory: (<span name="scala.Seq" class="extype">Seq</span>[(<a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>, <a href="../../nn/PTag.html" name="lamp.nn.PTag" id="lamp.nn.PTag" class="extype">PTag</a>)]) =&gt; <a href="../../nn/Optimizer.html" name="lamp.nn.Optimizer" id="lamp.nn.Optimizer" class="extype">Optimizer</a></span>, <span name="maxEpochs">maxEpochs: <span name="scala.Int" class="extype">Int</span></span>, <span name="checkpointState">checkpointState: <span name="scala.Option" class="extype">Option</span>[(<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>, <span name="lamp.data.distributed.localDataParallelTrainingLoop.LRState" class="extype">LRState</span>) =&gt; <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Unit" class="extype">Unit</span>]] = <span class="symbol">None</span></span>, <span name="validationFrequency">validationFrequency: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>, <span name="returnMinValidationLossModel">returnMinValidationLossModel: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Int" class="extype">Int</span>] = <span class="symbol">Nil</span></span>, <span name="learningRateSchedule">learningRateSchedule: <a href="../../nn/LearningRateSchedule.html" name="lamp.nn.LearningRateSchedule" id="lamp.nn.LearningRateSchedule" class="extype">LearningRateSchedule</a>[<span name="lamp.data.distributed.localDataParallelTrainingLoop.LRState" class="extype">LRState</span>] = <span class="symbol"><span class="name"><a href="../../index.html">LearningRateSchedule.noop</a></span></span></span>, <span name="initState">initState: <span name="scala.Option" class="extype">Option</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>] = <span class="symbol">None</span></span>, <span name="accumulateGradientOverNBatches">accumulateGradientOverNBatches: <span name="scala.Int" class="extype">Int</span> = <span class="symbol">1</span></span>, <span name="learningRateScheduleInitState">learningRateScheduleInitState: <span name="scala.Option" class="extype">Option</span>[<span name="lamp.data.distributed.localDataParallelTrainingLoop.LRState" class="extype">LRState</span>] = <span class="symbol">None</span></span>, <span name="validationLossExponentialSmoothingFactor">validationLossExponentialSmoothingFactor: <span name="scala.Double" class="extype">Double</span> = <span class="symbol">1.0</span></span>)</span><span class="result">: <span name="cats.effect.IO" class="extype">IO</span>[<a href="LoopState.html" name="lamp.data.distributed.LoopState" id="lamp.data.distributed.LoopState" class="extype">LoopState</a>]</span></span><p class="shortcomment cmt">Data parallel training loop driving multiple devices from a single process</p><div class="fullcomment"><div class="comment cmt"><p>Data parallel training loop driving multiple devices from a single process</p><p><code>modelsWithDataStreams</code> is sequence of models, training and validation
streams allocated to each devices. The streams must have the same length
and must not contain empty batches. Models must have the same shape.</p><p>Once the returned suspended side effect is completed the trained model is
in the *first* model of <code>modelsWithDataStreams</code>.
</p></div></div></li><li class="indented0 " name="lamp.data.distributed#oneEpoch" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="oneEpoch[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],S,C](model:lamp.nn.SupervisedModel[I,M],stepOptimizerFn:Option[Seq[Option[lamp.STen]]=&gt;Unit],batches:lamp.data.BatchStream[(I,lamp.STen),S,C],logger:Option[scribe.Logger],accumulateGradientOverNBatches:Int,ncclComm:aten.NcclComm,rootRank:Int,device:lamp.CudaDevice,forwardOnly:Boolean):cats.effect.IO[Double]" class="anchorToMember"></a><a id="oneEpoch[I,M&lt;:GenericModule[I,Variable],S,C](SupervisedModel[I,M],Option[(Seq[Option[STen]])=&gt;Unit],BatchStream[(I,STen),S,C],Option[Logger],Int,NcclComm,Int,CudaDevice,Boolean):IO[Double]" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/index.html#oneEpoch[I,M&lt;:lamp.nn.GenericModule[I,lamp.autograd.Variable],S,C](model:lamp.nn.SupervisedModel[I,M],stepOptimizerFn:Option[Seq[Option[lamp.STen]]=&gt;Unit],batches:lamp.data.BatchStream[(I,lamp.STen),S,C],logger:Option[scribe.Logger],accumulateGradientOverNBatches:Int,ncclComm:aten.NcclComm,rootRank:Int,device:lamp.CudaDevice,forwardOnly:Boolean):cats.effect.IO[Double]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">oneEpoch</span><span class="tparams">[<span name="I">I</span>, <span name="M">M &lt;: <a href="../../nn/GenericModule.html" name="lamp.nn.GenericModule" id="lamp.nn.GenericModule" class="extype">GenericModule</a>[<span name="lamp.data.distributed.oneEpoch.I" class="extype">I</span>, <a href="../../autograd/Variable.html" name="lamp.autograd.Variable" id="lamp.autograd.Variable" class="extype">Variable</a>]</span>, <span name="S">S</span>, <span name="C">C</span>]</span><span class="params">(<span name="model">model: <a href="../../nn/SupervisedModel.html" name="lamp.nn.SupervisedModel" id="lamp.nn.SupervisedModel" class="extype">SupervisedModel</a>[<span name="lamp.data.distributed.oneEpoch.I" class="extype">I</span>, <span name="lamp.data.distributed.oneEpoch.M" class="extype">M</span>]</span>, <span name="stepOptimizerFn">stepOptimizerFn: <span name="scala.Option" class="extype">Option</span>[(<span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Option" class="extype">Option</span>[<a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>]]) =&gt; <span name="scala.Unit" class="extype">Unit</span>]</span>, <span name="batches">batches: <a href="../BatchStream.html" name="lamp.data.BatchStream" id="lamp.data.BatchStream" class="extype">BatchStream</a>[(<span name="lamp.data.distributed.oneEpoch.I" class="extype">I</span>, <a href="../../STen.html" name="lamp.STen" id="lamp.STen" class="extype">STen</a>), <span name="lamp.data.distributed.oneEpoch.S" class="extype">S</span>, <span name="lamp.data.distributed.oneEpoch.C" class="extype">C</span>]</span>, <span name="logger">logger: <span name="scala.Option" class="extype">Option</span>[<span name="scribe.Logger" class="extype">Logger</span>]</span>, <span name="accumulateGradientOverNBatches">accumulateGradientOverNBatches: <span name="scala.Int" class="extype">Int</span></span>, <span name="ncclComm">ncclComm: <span name="aten.NcclComm" class="extype">NcclComm</span></span>, <span name="rootRank">rootRank: <span name="scala.Int" class="extype">Int</span></span>, <span name="device">device: <a href="../../CudaDevice.html" name="lamp.CudaDevice" id="lamp.CudaDevice" class="extype">CudaDevice</a></span>, <span name="forwardOnly">forwardOnly: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="cats.effect.IO" class="extype">IO</span>[<span name="scala.Double" class="extype">Double</span>]</span></span><p class="shortcomment cmt">Drives one epoch in the clique All batch streams in all members of the
clique *MUST* have the same number of batches otherwise this will never
terminate
</p></li><li class="indented0 " name="lamp.data.distributed.DistributedCommunication" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="DistributedCommunication" class="anchorToMember"></a><a id="DistributedCommunication:DistributedCommunication" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/DistributedCommunication$.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">object</span></span> <span class="symbol"><a href="DistributedCommunication$.html" title=""><span class="name">DistributedCommunication</span></a></span></li><li class="indented0 " name="lamp.data.distributed.LocalCommunication" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="LocalCommunication" class="anchorToMember"></a><a id="LocalCommunication:LocalCommunication" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/LocalCommunication$.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">object</span></span> <span class="symbol"><a href="LocalCommunication$.html" title=""><span class="name">LocalCommunication</span></a></span></li><li class="indented0 " name="lamp.data.distributed.LoopState" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="LoopState" class="anchorToMember"></a><a id="LoopState:LoopState" class="anchorToMember"></a> <span class="permalink"><a href="../../../lamp/data/distributed/LoopState$.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">object</span></span> <span class="symbol"><a href="LoopState$.html" title=""><span class="name">LoopState</span></a><span class="result"> extends <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a></span></span></li></ol></div></div><div id="inheritedMembers"><div name="scala.AnyRef" class="parent"><h3>Inherited from <span name="scala.AnyRef" class="extype">AnyRef</span></h3></div><div name="scala.Any" class="parent"><h3>Inherited from <span name="scala.Any" class="extype">Any</span></h3></div></div><div id="groupedMembers"><div name="Ungrouped" class="group"><h3>Ungrouped</h3></div></div></div><div id="tooltip"></div><div id="footer"></div></body></div></div></div></body></html>
